# Summary

This project loads the sparkify user activity app logs from S3 files into spark memory and writes the transformed data back to s3 in a parquet file format as fact and dimension tables.  
Data is processed in a way that the data can be loaded from parquet file into any visualization tools or an AWS service like athena for running different queries on demand.

# Input Data Set

Following data is the input dataset available in S3 Song data (s3://udacity-dend/song_data) and Log data (s3://udacity-dend/log_data)

Song Dataset is a subset of real data from the Million Song Dataset. It's a JSON contains metadata about a song and artist of that song. 

Log Dataset consist of imaginary streaming app (Sparkify) log files in a JSON format generated by event simulator. Log files are partitioned by year and Month

# Spark Data processing
Song and Log data is loaded into Spark Memory by providing the appropriate schema and once the data is loaded into spark data frame a temporary view is created to execute spark sql.

Spark sql selects the appropriate fields required for all the fact and dimension tables and also performed data transformations where needed.

Following are the few important data transformations are done via spark sql 
1. User id is converted into integer data type using CAST function.
2. Also used from_unixtime to convert the epoch time into specific format (hour, day, month, year, weekday).
3. Used a user defined function (udf) to convert the epoch time into approrpiate week of the year.

# Parquet Files
Parquet stores data in a flat columnar format. Parquet is more efficient in terms of storage and performance compared to row-oriented approach.
Spark dataframes are written in to Parquet file format and applied neccessary partition when writing to parquet format.

# Python Script
etl.py - Loads the data from S3 into Spark memory and transforms the data in memory and loads back into parquet files.

    Local mode Execution command : spark-submit etl.py 
    Cluster Command used in EMR : spark-submit --master yarn --deploy-mode cluster --num-executors 16 --files dl.cfg  etl.py


